{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCou9Pbnha3U20ibIWNKE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renan-Domingues/IntroductionToPytorch/blob/main/IntroductionPyTorch_04_BuildingModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILDING MODELS WITH PYTORCH\n"
      ],
      "metadata": {
        "id": "3R9C1XoUUdu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch.nn.Module and torch.nn.Parameter\n",
        "we'll be discussing sime of the tools Pytorch makes available for buiding a deep learning network.\n",
        "\n",
        "One important behavior of torch.nn.Module is registering parameters. If a particular Module subclass has learning weights, these weights are expressed as instances of torch.nn.Parameter. The Parameter class is a subclass of torch.Tensor, with the special behavior that when they are assigned as attributes of a Module, they are added to the list of that modules parameters. These parameters may be accessed through the parameters() method on the Module class."
      ],
      "metadata": {
        "id": "Nln8x5rtUky6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''As a example, here is a model with 2 linear layers and a activation function\n",
        "we will create a instance of it and ask it to report on its parameters'''\n",
        "\n",
        "import torch\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TinyModel, self).__init__()\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(100, 200)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.linear2 = torch.nn.Linear(200, 10)\n",
        "    self.softmax = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The model')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "  print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHxxz_KSVckt",
        "outputId": "d2128bef-5ded-4100-c0c7-3645b39a6b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model\n",
            "TinyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            "Layer params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0429, -0.0662, -0.0542,  ...,  0.0189, -0.0415,  0.0284],\n",
            "        [-0.0472,  0.0661, -0.0199,  ..., -0.0425,  0.0031, -0.0400],\n",
            "        [ 0.0689,  0.0447,  0.0259,  ..., -0.0540,  0.0680,  0.0206],\n",
            "        ...,\n",
            "        [-0.0284, -0.0210,  0.0281,  ...,  0.0653, -0.0271,  0.0276],\n",
            "        [-0.0269,  0.0350,  0.0024,  ...,  0.0540, -0.0534,  0.0679],\n",
            "        [-0.0351, -0.0388,  0.0703,  ..., -0.0186, -0.0624, -0.0179]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0282,  0.0406, -0.0447,  0.0329, -0.0313, -0.0492, -0.0702, -0.0259,\n",
            "        -0.0412, -0.0439], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the fundamental structure of a PyTorchModel: there is a __init__()method that defines the layers and other components of a model, and a forward() method where the computation gets done.\n",
        "\n",
        "(note that we can print the model, or any of its submodules, to learn about the structure)"
      ],
      "metadata": {
        "id": "JGQCK0csXkG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Layer Typres\n"
      ],
      "metadata": {
        "id": "q7ABWLzTZa0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Layers\n",
        "\n",
        "The most basic type of a neural network layer,  is  a linear or fully connected layer.\n",
        "This is a layer where every input influences every output of the layer to a degree specified by the layer's weights. If the model has \"m\" inputs and \"n\" outputs, the weights will be an m * n matrix."
      ],
      "metadata": {
        "id": "IAHpEp-XZjQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for example\n",
        "\n",
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(1, 3)\n",
        "print(f'Input {x}')\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "  print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print(f'\\n\\nOutput: {y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlYcBsaxaWOZ",
        "outputId": "e114def7-a08c-4d11-bbaa-fd5dc71e5081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor([[0.5920, 0.2046, 0.8131]])\n",
            "\n",
            "\n",
            "Weight and Bias parameters:\n",
            "Parameter containing:\n",
            "tensor([[ 0.4862,  0.4826,  0.2157],\n",
            "        [ 0.4033,  0.3856, -0.1862]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2338, -0.3974], requires_grad=True)\n",
            "\n",
            "\n",
            "Output: tensor([[ 0.3281, -0.2311]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the matrix multiplication of \"x\" by the linear layer's weights, and add the biases, is the output vector \"y\"\n",
        "\n",
        "the ``lin.weight`` is reported itself as Parameter (Parameter is a subclass of Tensor), and let us know that it's tracking gradients with autograd. This is a default behavior for Parameter that differs from Tensor."
      ],
      "metadata": {
        "id": "aSAZ5r1Bbc14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layers\n",
        "\n",
        "they are build to handle data with a high degree of spatial correlation, very commonly use in computer vision, where they detect close groupings of features which the compode into higher-level features.\n",
        "they are appear in NPL (Natural Language Processing (artificial intelligence)) aplication, where word's immediate context can affect the meaning of a sentence.\n"
      ],
      "metadata": {
        "id": "m7iHNflkcgNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We saw convolutional layers early on in a LeNet module\n",
        "\n",
        "import torch.functional as F\n",
        "\n",
        "class LeNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "\n",
        "    # 1 input image channel (Black & White), 6 output channels, 5x5 square convolutional\n",
        "\n",
        "    self.conv1 = torch.nn.Conv2d(1, 6, 5) # 1st is the number of color channels, 2nd is to learn 6 features(output features) and 3rd image size (5x5)\n",
        "    self.conv2 = torch.nn.Conv2d(6, 16, 3) # 1st expect 6 input channels (correspondig the 6 features of the first layer), 2nd 16 input channels and 3rd 3x3 kernel\n",
        "\n",
        "    self.fc1 = torch.nn.Linear(16 * 6 * 6, 120) # 6*6 image dimension\n",
        "    self.fc2 = torch.nn.Linear(120, 84)\n",
        "    self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # max pooling over 2, 2 window\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    x = x.view(-1, self.num_flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "  def num_flat_features(self, x):\n",
        "    size = x.size()[1:] # all dimensions except the batch dimension\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features"
      ],
      "metadata": {
        "id": "AzGoDO7veOcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "more details in  a break  down of this  model:  https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html"
      ],
      "metadata": {
        "id": "kQ1J26esiM1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More aplications about Convolution Layers: https://pytorch.org/docs/stable/nn.html#convolution-layers"
      ],
      "metadata": {
        "id": "rm6vESPHkQ4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent Layers\n",
        "Recurrent neural networks (or RNNs) are used for sequential data. An RNN does this by maintaining a hidden state that acts as a sort of memory for what it has seen in the sequence so far.\n",
        "\n",
        "The internal structure of an RNN layer - or its variants, the LSTM (long short-term memory) and GRU (gated recurrent unit) - is moderately complex (so it's not part of this larning class).\n",
        "but  let's see what one looks like"
      ],
      "metadata": {
        "id": "lRtbwbf2kbrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(torch.nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "    self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "metadata": {
        "id": "h1UoQ_aAlN1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is too advanced for the atual state of learning\n",
        "so we are not going through the details\n",
        "\n",
        "but the documentation for Sequence Models and LSTM Networks is here: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
      ],
      "metadata": {
        "id": "SqUGfmFZmoUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers\n",
        "Transformers are multi-purpose networks that have taken over the state of the art in NLP with models like BERT.\n",
        "\n",
        "\n",
        "NPL = Natural Language Processing, common in AI\n",
        "\n",
        "PyTorch has a Transformer class that allows you to define the overall parameters of a transformer model - the number of attention heads, the number of encoder & decoder layers, dropout and activation functions, etc.\n",
        "\n",
        "documantation to Transformers: https://pytorch.org/docs/stable/nn.html#transformer-layers\n",
        "\n",
        "and tutorial: https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
      ],
      "metadata": {
        "id": "JlGqtMb-nB7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Layers and Functions"
      ],
      "metadata": {
        "id": "sAs9B4TGryIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data manipulation Layers\n",
        "There are other layer types that perform important functions in models, but don't participate in the learning process themselves.\n",
        "\n",
        "- Max pooling(and its twin, min pooling) reduce a tensor by combining cells, and assigning the maximum value of the input cells o the output cell"
      ],
      "metadata": {
        "id": "5bkSMoX3rz0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for example\n",
        "my_tensor = torch.rand(1, 6, 6)\n",
        "print(my_tensor)\n",
        "\n",
        "maxpool_layer = torch.nn.MaxPool2d(3)\n",
        "print(maxpool_layer(my_tensor))\n",
        "\n",
        "print(f'My_tensor: {my_tensor.shape} | Maxpool: {maxpool_layer(my_tensor).shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVaKirqmuKCA",
        "outputId": "905c41d3-4dde-4308-e3b9-2d6edca7e72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.9032, 0.9015, 0.3176, 0.0506, 0.4068, 0.3693],\n",
            "         [0.8099, 0.7067, 0.5140, 0.2802, 0.4934, 0.7527],\n",
            "         [0.1873, 0.1097, 0.0329, 0.4705, 0.7273, 0.7777],\n",
            "         [0.2521, 0.2762, 0.2899, 0.4091, 0.1444, 0.9342],\n",
            "         [0.0169, 0.6084, 0.0200, 0.3301, 0.5943, 0.0153],\n",
            "         [0.1782, 0.2835, 0.1359, 0.5681, 0.1155, 0.8131]]])\n",
            "tensor([[[0.9032, 0.7777],\n",
            "         [0.6084, 0.9342]]])\n",
            "My_tensor: torch.Size([1, 6, 6]) | Maxpool: torch.Size([1, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of the values in the maxpooled output is the maximum value of each quadrant of the 6x6 input."
      ],
      "metadata": {
        "id": "1B5JovrjvGxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Normalization layers = re-center and normalize the output of one layer before feeding it to another. Centering the and scaling the intermediate tensors has a number of beneficial effects, such as letting you use higher learning rates without exploding/vanishing gradients."
      ],
      "metadata": {
        "id": "kQfNDQ6ZvqvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
        "print(my_tensor)\n",
        "\n",
        "print(my_tensor.mean())\n",
        "\n",
        "norm_layer = torch.nn.BatchNorm1d(4)\n",
        "normed_tensor = norm_layer(my_tensor)\n",
        "print(normed_tensor)\n",
        "\n",
        "print(normed_tensor.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zXhDsZevzZI",
        "outputId": "225639e2-5b48-4cb8-c32f-4d503eaaf2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 7.9611,  8.7461, 17.5208,  9.4717],\n",
            "         [14.7659,  7.1994,  7.6848, 10.2895],\n",
            "         [ 6.7386, 17.0155, 16.3440, 13.5909],\n",
            "         [19.6604, 15.5439, 17.4119, 20.3279]]])\n",
            "tensor(13.1420)\n",
            "tensor([[[-0.7707, -0.5666,  1.7153, -0.3779],\n",
            "         [ 1.5937, -0.9285, -0.7667,  0.1015],\n",
            "         [-1.6436,  0.8836,  0.7185,  0.0415],\n",
            "         [ 0.7525, -1.4223, -0.4354,  1.1051]]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "tensor(6.7055e-08, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalized tensors is a smaller number, grouped aroud zero. This is beneficial because many activation functions have their strongest gradients near 0, but sometimes suffer from vanishing or exploding gradients for inputs that drive them far away from zero"
      ],
      "metadata": {
        "id": "DL6k_NyDwmTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout Layers = are a tool for encouraging sparse representations in your model - that is, pushing it to do inference with less data.\n",
        "\n",
        "Dropout layers work by randomly setting parts of the input tensor during training - dropout layers are always turned off for inference. This forces the model to learn against this masked or reduced dataset. For example:"
      ],
      "metadata": {
        "id": "CYdMpyZxxm2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 4, 4)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.4)\n",
        "print(dropout(my_tensor))\n",
        "print(dropout(my_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb0nNA-Tx2Gl",
        "outputId": "56f3dddf-ca4c-4167-86f3-82794708ac94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0000, 0.2500, 0.9918, 1.6259],\n",
            "         [1.0676, 0.4539, 0.2993, 0.0000],\n",
            "         [0.0000, 1.4900, 1.2566, 1.2189],\n",
            "         [1.2045, 0.0000, 0.3699, 1.1011]]])\n",
            "tensor([[[0.4019, 0.2500, 0.9918, 0.0000],\n",
            "         [1.0676, 0.0000, 0.2993, 0.0000],\n",
            "         [0.2511, 1.4900, 0.0000, 1.2189],\n",
            "         [0.0000, 1.4670, 0.0000, 0.0000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Activation Functions\n",
        "\n",
        "Activation function make deep learning possible. A neural network is a program with many parameters that simulates mathematical function.\n",
        "\n",
        "Inserting non-linear activation functions between layers is what allows a deep learning model to simulate any function.\n",
        "\n",
        "torch.nn.Module has objects encapsulating all of the major activation functions including ReLU and its many variants, Tanh, Hardtanh, sigmoid, and more. It also includes other functions, such as Softmax, that are most useful at the output stage of a model."
      ],
      "metadata": {
        "id": "-mjkr_ygyNKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "The loss function tell us how far a model's predction is from the correct answer"
      ],
      "metadata": {
        "id": "l54LhPSu2gES"
      }
    }
  ]
}